# Deep Q Network For BeerGame


• Implemented Q-learning with Deep Neural Networks to approximate Q-values,target network
to to provide steady Q-value predictions, experience replay buffer to stabilize and improve <br/>
the efficiency, and shaped reward to prioritize system-wide objectives over local ones
• Transfer learning is applied to reduce training time for different agents <br/>
• achieved near-optimal order quantities when playing with agents using base-stock policies <br/>
and surpassed base-stock policies with Sterman co-players.

Incorporated code from Afshin Oroojlooy’s repository, accessible at github, for my research.<br/>
Initially, encountering this repository was both exciting and challenging.<br/>
The provided code did not function as intended at first, prompting me to deeply understand and enhance its functionality and made several
enhancements and modifications.<br/>
This repository hosts the refined version of the code with enhancements to ensure its operational integrity for scholarly or developmental pursuits.<br/>

Supervisor: Prof.Erfan Hassannaebi <br/>
main reference: https://doi.org/10.48550/arXiv.1708.05924

